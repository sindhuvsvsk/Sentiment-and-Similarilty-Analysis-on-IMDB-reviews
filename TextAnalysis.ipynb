{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis on movie reviews\n",
    "## Sentiment Analysis\n",
    "To Run this notebook below libraries should be installed:\n",
    ">conda install -c conda-forge textblob\n",
    "\n",
    ">ipython -m textblob.download_corpora\n",
    "\n",
    ">conda install -c conda-forge spacy\n",
    "\n",
    ">ipython -m spacy download en\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created a dataset to perform sentiment analysis on imdb movie reviews and loaded the data into dataframe.This dataset has two dozen reviews and each review is labeled with star rating of range(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Reviewer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>There are some vague references to things that...</td>\n",
       "      <td>James Berardinelli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>It must have taken a superhuman effort to pull...</td>\n",
       "      <td>JEFFREY M. ANDERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Considered on its own, as a single, nearly 2-h...</td>\n",
       "      <td>A.O. Scott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>If a movie can be defined by the strength of i...</td>\n",
       "      <td>Louise Keller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Ten years and 18 Marvel movies; 19 now, with t...</td>\n",
       "      <td>MaryAnn Johanson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review  \\\n",
       "0       9  There are some vague references to things that...   \n",
       "1       7  It must have taken a superhuman effort to pull...   \n",
       "2       7  Considered on its own, as a single, nearly 2-h...   \n",
       "3       8  If a movie can be defined by the strength of i...   \n",
       "4       7  Ten years and 18 Marvel movies; 19 now, with t...   \n",
       "\n",
       "              Reviewer  \n",
       "0   James Berardinelli  \n",
       "1  JEFFREY M. ANDERSON  \n",
       "2          Â A.O. Scott  \n",
       "3        Louise Keller  \n",
       "4     MaryAnn Johanson  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movie= pd.read_csv('Avengers.csv', delimiter = ',' , encoding = 'unicode_escape') \n",
    "movie.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating       int64\n",
       "Review      object\n",
       "Reviewer    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.dtypes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing TextBlob's Default Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentiment(polarity=0.07485954785954785, subjectivity=0.4041014911014911), Sentiment(polarity=0.003187003968253964, subjectivity=0.6010540674603175), Sentiment(polarity=0.03743156092541338, subjectivity=0.5117808648546354), Sentiment(polarity=0.06123511904761904, subjectivity=0.5096301020408164), Sentiment(polarity=0.0866197079756402, subjectivity=0.496341135324186), Sentiment(polarity=0.18053641047062097, subjectivity=0.5534287106655527), Sentiment(polarity=0.09241730752907225, subjectivity=0.4659476445123504), Sentiment(polarity=0.05796594982078853, subjectivity=0.514925755248336), Sentiment(polarity=0.11358252646420584, subjectivity=0.5281944306753467), Sentiment(polarity=0.09820963541666665, subjectivity=0.417133246527778), Sentiment(polarity=0.11303699736689428, subjectivity=0.5272655866470299), Sentiment(polarity=0.15156377028926044, subjectivity=0.5010124116986863), Sentiment(polarity=0.11689213564213566, subjectivity=0.4731673881673883), Sentiment(polarity=0.030963876863876864, subjectivity=0.4016429052429052), Sentiment(polarity=0.22810077519379846, subjectivity=0.5766957364341084), Sentiment(polarity=0.13140480895915677, subjectivity=0.5045527218353306), Sentiment(polarity=0.1183314732142857, subjectivity=0.4681175595238095), Sentiment(polarity=0.029071604071604067, subjectivity=0.5512441012441012), Sentiment(polarity=0.12752525252525254, subjectivity=0.44530676220331394), Sentiment(polarity=0.11881561147186148, subjectivity=0.5484362974987974), Sentiment(polarity=0.05754240518391462, subjectivity=0.44931865828092254), Sentiment(polarity=0.08286830357142858, subjectivity=0.40636160714285724), Sentiment(polarity=0.1299342105263158, subjectivity=0.5822368421052633), Sentiment(polarity=0.17883802705231275, subjectivity=0.43019995877138734)]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "Review = movie['Review']\n",
    "blobs = [TextBlob(Review[i]) for i in range(movie.shape[0])]\n",
    "blob_sentiment=[blob.sentiment for blob in blobs]\n",
    "print(blob_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blob.sentiment.polarity     = 0.075\n",
      "blob.sentiment.subjectivity = 0.404\n",
      "\n",
      "blob.sentiment.polarity     = 0.003\n",
      "blob.sentiment.subjectivity = 0.601\n",
      "\n",
      "blob.sentiment.polarity     = 0.037\n",
      "blob.sentiment.subjectivity = 0.512\n",
      "\n",
      "blob.sentiment.polarity     = 0.061\n",
      "blob.sentiment.subjectivity = 0.510\n",
      "\n",
      "blob.sentiment.polarity     = 0.087\n",
      "blob.sentiment.subjectivity = 0.496\n",
      "\n",
      "blob.sentiment.polarity     = 0.181\n",
      "blob.sentiment.subjectivity = 0.553\n",
      "\n",
      "blob.sentiment.polarity     = 0.092\n",
      "blob.sentiment.subjectivity = 0.466\n",
      "\n",
      "blob.sentiment.polarity     = 0.058\n",
      "blob.sentiment.subjectivity = 0.515\n",
      "\n",
      "blob.sentiment.polarity     = 0.114\n",
      "blob.sentiment.subjectivity = 0.528\n",
      "\n",
      "blob.sentiment.polarity     = 0.098\n",
      "blob.sentiment.subjectivity = 0.417\n",
      "\n",
      "blob.sentiment.polarity     = 0.113\n",
      "blob.sentiment.subjectivity = 0.527\n",
      "\n",
      "blob.sentiment.polarity     = 0.152\n",
      "blob.sentiment.subjectivity = 0.501\n",
      "\n",
      "blob.sentiment.polarity     = 0.117\n",
      "blob.sentiment.subjectivity = 0.473\n",
      "\n",
      "blob.sentiment.polarity     = 0.031\n",
      "blob.sentiment.subjectivity = 0.402\n",
      "\n",
      "blob.sentiment.polarity     = 0.228\n",
      "blob.sentiment.subjectivity = 0.577\n",
      "\n",
      "blob.sentiment.polarity     = 0.131\n",
      "blob.sentiment.subjectivity = 0.505\n",
      "\n",
      "blob.sentiment.polarity     = 0.118\n",
      "blob.sentiment.subjectivity = 0.468\n",
      "\n",
      "blob.sentiment.polarity     = 0.029\n",
      "blob.sentiment.subjectivity = 0.551\n",
      "\n",
      "blob.sentiment.polarity     = 0.128\n",
      "blob.sentiment.subjectivity = 0.445\n",
      "\n",
      "blob.sentiment.polarity     = 0.119\n",
      "blob.sentiment.subjectivity = 0.548\n",
      "\n",
      "blob.sentiment.polarity     = 0.058\n",
      "blob.sentiment.subjectivity = 0.449\n",
      "\n",
      "blob.sentiment.polarity     = 0.083\n",
      "blob.sentiment.subjectivity = 0.406\n",
      "\n",
      "blob.sentiment.polarity     = 0.130\n",
      "blob.sentiment.subjectivity = 0.582\n",
      "\n",
      "blob.sentiment.polarity     = 0.179\n",
      "blob.sentiment.subjectivity = 0.430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for blob in blobs:\n",
    "    print(f'blob.sentiment.polarity     = {blob.sentiment.polarity:.3f}')\n",
    "    print(f'blob.sentiment.subjectivity = {blob.sentiment.subjectivity:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Naive Bayes Analyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='pos', p_pos=1.0, p_neg=1.0564645454909183e-31)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=6.167978887476067e-19)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=1.167853579450459e-30)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=4.3447663136914435e-15)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=1.3741068787334962e-16)\n",
      "Sentiment(classification='pos', p_pos=0.9999992859619545, p_neg=7.140380082742279e-07)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=2.4494219843459404e-19)\n",
      "Sentiment(classification='pos', p_pos=0.9999999948492838, p_neg=5.1507143608269886e-09)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=1.810875464976536e-40)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=3.1361347066222123e-20)\n",
      "Sentiment(classification='pos', p_pos=0.9999999820571658, p_neg=1.7942888402876744e-08)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=2.7391185437548916e-21)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=8.842627161690769e-36)\n",
      "Sentiment(classification='pos', p_pos=0.999990648596292, p_neg=9.351403694959593e-06)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=1.8499183752639595e-23)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=2.904180361834758e-33)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=2.8542251358492447e-28)\n",
      "Sentiment(classification='pos', p_pos=0.9999999999963751, p_neg=3.593373811076532e-12)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=1.664893481449654e-24)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=3.133394440710198e-20)\n",
      "Sentiment(classification='pos', p_pos=0.9999999527384386, p_neg=4.7261557089364594e-08)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=7.085203298682783e-21)\n",
      "Sentiment(classification='pos', p_pos=0.9999999703131106, p_neg=2.9686877138244532e-08)\n",
      "Sentiment(classification='pos', p_pos=1.0, p_neg=1.3001913506429465e-17)\n"
     ]
    }
   ],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "for i in range(movie.shape[0]):\n",
    "    blob = TextBlob(Review[i], analyzer=NaiveBayesAnalyzer())\n",
    "    print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "1. The output of Sentiment Analyzer is categorized between polarity and subjectivity for each review.\n",
    "2. Based on the analysis it shows that polarity score for all the reviews is found to be greater than zero, hence we can classify the reviews for this movie as **positive**.\n",
    "3. The subjectivity score is a float within the range [0.0,1.0] where 0.0 is very objective and 1.0 is very subjective. Subjective sentence expresses some personal feelings, views, beliefs, opinions, allegations, desires, beliefs, suspicions, and speculations where as Objective sentences are factual. Here for our data the score ranges between (0.4,0.6) hence we can classify it as neither subjective nor objective. \n",
    "4. The sentiment analysis using Naive Bayes Analyzer,that is trained on data of movie reviews will predict whether the review is positive or negative. From the analysis, the overall sentiment score came out to be **positive** for all reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the text data into numerical format i.e. machine understandable format as the features must be numeric in any model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Sentiment_count=movie.groupby('Rating').count()\n",
    "#Sentiment_count['Review']\n",
    "plt.bar(Sentiment_count.index.values, Sentiment_count['Review'])\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#tokenizer to remove unwanted characters from data l\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing text data as BagOfWords(BoW)\n",
    "The next step is to create a numerical feature vector for each review. BoW counts the number of times that tokens appear in each review of the collection. It returns a matrix with the characteristics:\n",
    ">Number of columns = number of unique tokens in the whole collection.\n",
    ">Number of rows = number of documents in the whole collection of documents.\n",
    ">Every cell contains the frequency of a particular token (column) in a particular document (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3163)\t1\n",
      "  (0, 2377)\t1\n",
      "  (0, 2973)\t7\n",
      "  (0, 1359)\t2\n",
      "  (0, 3157)\t2\n",
      "  (0, 842)\t1\n",
      "  (0, 2172)\t2\n",
      "  (0, 948)\t1\n",
      "  (0, 233)\t1\n",
      "  (0, 3045)\t1\n",
      "  (0, 2224)\t1\n",
      "  (0, 2388)\t1\n",
      "  (0, 541)\t1\n",
      "  (0, 3060)\t1\n",
      "  (0, 2395)\t1\n",
      "  (0, 2753)\t1\n",
      "  (0, 1709)\t1\n",
      "  (0, 2203)\t1\n",
      "  (0, 599)\t2\n",
      "  (0, 1766)\t1\n",
      "  (0, 3195)\t1\n",
      "  (0, 1054)\t2\n",
      "  (0, 238)\t1\n",
      "  (0, 2350)\t1\n",
      "  (0, 2568)\t1\n",
      "  :\t:\n",
      "  (23, 334)\t1\n",
      "  (23, 210)\t1\n",
      "  (23, 3085)\t1\n",
      "  (23, 1217)\t1\n",
      "  (23, 826)\t1\n",
      "  (23, 1789)\t1\n",
      "  (23, 952)\t1\n",
      "  (23, 2065)\t1\n",
      "  (23, 914)\t1\n",
      "  (23, 694)\t1\n",
      "  (23, 2369)\t1\n",
      "  (23, 1013)\t1\n",
      "  (23, 1398)\t1\n",
      "  (23, 1111)\t2\n",
      "  (23, 373)\t1\n",
      "  (23, 1164)\t1\n",
      "  (23, 3008)\t1\n",
      "  (23, 2113)\t1\n",
      "  (23, 830)\t1\n",
      "  (23, 1696)\t1\n",
      "  (23, 1595)\t1\n",
      "  (23, 2426)\t1\n",
      "  (23, 151)\t1\n",
      "  (23, 454)\t1\n",
      "  (23, 255)\t1\n"
     ]
    }
   ],
   "source": [
    "# We use the class CountVectorizer from scikit-learn library.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(movie['Review'])\n",
    "print(text_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Vocabulary and freqencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 1),\n",
       " ('00pm', 1),\n",
       " ('1', 2),\n",
       " ('10', 12),\n",
       " ('100', 2),\n",
       " ('10th', 1),\n",
       " ('12', 2),\n",
       " ('13', 2),\n",
       " ('149', 2),\n",
       " ('150', 1),\n",
       " ('156', 3),\n",
       " ('18', 9),\n",
       " ('19', 4),\n",
       " ('1963', 1),\n",
       " ('1991', 1),\n",
       " ('1992', 1),\n",
       " ('19th', 5),\n",
       " ('2', 6),\n",
       " ('20', 3),\n",
       " ('2008', 4),\n",
       " ('2012', 1),\n",
       " ('2013', 2),\n",
       " ('2014', 2),\n",
       " ('2015', 1),\n",
       " ('2016', 3),\n",
       " ('2018', 3),\n",
       " ('2019', 4),\n",
       " ('21st', 1),\n",
       " ('28', 2),\n",
       " ('3', 1),\n",
       " ('30', 2),\n",
       " ('4', 3),\n",
       " ('40', 2),\n",
       " ('5', 2),\n",
       " ('50', 1),\n",
       " ('64', 1),\n",
       " ('7', 1),\n",
       " ('8', 1),\n",
       " ('9', 1),\n",
       " ('abandoning', 1),\n",
       " ('abilities', 2),\n",
       " ('ability', 2),\n",
       " ('able', 7),\n",
       " ('absences', 1),\n",
       " ('absent', 2),\n",
       " ('absolutely', 3),\n",
       " ('abundance', 1),\n",
       " ('academy', 2),\n",
       " ('accent', 2),\n",
       " ('acceptance', 1),\n",
       " ('accepting', 1),\n",
       " ('accidents', 1),\n",
       " ('accompanied', 1),\n",
       " ('accompany', 1),\n",
       " ('accomplish', 3),\n",
       " ('accomplished', 3),\n",
       " ('according', 2),\n",
       " ('accordingly', 1),\n",
       " ('account', 3),\n",
       " ('accustomed', 3),\n",
       " ('achieve', 2),\n",
       " ('achieved', 2),\n",
       " ('achievement', 4),\n",
       " ('achieving', 1),\n",
       " ('acquire', 3),\n",
       " ('acquired', 1),\n",
       " ('acquires', 1),\n",
       " ('acquiring', 2),\n",
       " ('act', 5),\n",
       " ('acting', 3),\n",
       " ('action', 39),\n",
       " ('actions', 2),\n",
       " ('actively', 1),\n",
       " ('actor', 2),\n",
       " ('actors', 3),\n",
       " ('actual', 4),\n",
       " ('actually', 6),\n",
       " ('adaptation', 1),\n",
       " ('adaptations', 1),\n",
       " ('adapts', 1),\n",
       " ('add', 2),\n",
       " ('added', 1),\n",
       " ('adding', 2),\n",
       " ('additionally', 1),\n",
       " ('address', 2),\n",
       " ('addressed', 1),\n",
       " ('adds', 1),\n",
       " ('adhered', 1),\n",
       " ('admirable', 1),\n",
       " ('admirably', 1),\n",
       " ('admit', 1),\n",
       " ('admittedly', 1),\n",
       " ('adopted', 6),\n",
       " ('adorable', 1),\n",
       " ('adults', 1),\n",
       " ('advance', 4),\n",
       " ('advanced', 2),\n",
       " ('adventures', 4),\n",
       " ('affiliated', 1),\n",
       " ('affixing', 2),\n",
       " ('afford', 2),\n",
       " ('affronted', 1),\n",
       " ('afloat', 1),\n",
       " ('aforementioned', 2),\n",
       " ('afraid', 2),\n",
       " ('afterthought', 1),\n",
       " ('age', 4),\n",
       " ('agent', 1),\n",
       " ('agents', 1),\n",
       " ('agility', 1),\n",
       " ('ago', 3),\n",
       " ('agree', 1),\n",
       " ('aha', 1),\n",
       " ('ahead', 1),\n",
       " ('ai', 1),\n",
       " ('aid', 1),\n",
       " ('aim', 2),\n",
       " ('aka', 2),\n",
       " ('akin', 1),\n",
       " ('alan', 3),\n",
       " ('alert', 1),\n",
       " ('alerted', 1),\n",
       " ('alien', 4),\n",
       " ('alien3', 1),\n",
       " ('alike', 1),\n",
       " ('allegorical', 2),\n",
       " ('allow', 5),\n",
       " ('allowed', 1),\n",
       " ('allowing', 2),\n",
       " ('allows', 1),\n",
       " ('alongside', 1),\n",
       " ('alpha', 1),\n",
       " ('alphabetical', 1),\n",
       " ('alter', 2),\n",
       " ('altogether', 1),\n",
       " ('amass', 1),\n",
       " ('amazing', 7),\n",
       " ('ambition', 2),\n",
       " ('ambitions', 1),\n",
       " ('ambitious', 2),\n",
       " ('america', 32),\n",
       " ('amounts', 1),\n",
       " ('analysis', 1),\n",
       " ('anchor', 1),\n",
       " ('anchored', 1),\n",
       " ('ancient', 1),\n",
       " ('android', 2),\n",
       " ('angry', 1),\n",
       " ('annihilating', 1),\n",
       " ('anniversary', 2),\n",
       " ('answer', 12),\n",
       " ('answered', 1),\n",
       " ('ant', 6),\n",
       " ('anthony', 19),\n",
       " ('anti', 1),\n",
       " ('anticipation', 1),\n",
       " ('anxious', 1),\n",
       " ('anymore', 1),\n",
       " ('apart', 3),\n",
       " ('aplenty', 1),\n",
       " ('aplomb', 1),\n",
       " ('apocalypse', 2),\n",
       " ('apocalyptic', 2),\n",
       " ('apparently', 4),\n",
       " ('appeal', 3),\n",
       " ('appeals', 1),\n",
       " ('appear', 1),\n",
       " ('appearance', 1),\n",
       " ('appearances', 1),\n",
       " ('appeared', 3),\n",
       " ('appearing', 2),\n",
       " ('appears', 2),\n",
       " ('applaud', 1),\n",
       " ('applauded', 1),\n",
       " ('applause', 1),\n",
       " ('appointed', 1),\n",
       " ('appreciate', 4),\n",
       " ('appreciated', 1),\n",
       " ('approach', 1),\n",
       " ('approaching', 1),\n",
       " ('appropriately', 1),\n",
       " ('arc', 6),\n",
       " ('arcs', 1),\n",
       " ('area', 1),\n",
       " ('aren', 8),\n",
       " ('arguably', 1),\n",
       " ('argue', 2),\n",
       " ('argument', 2),\n",
       " ('aria', 1),\n",
       " ('armageddon', 1),\n",
       " ('armed', 1),\n",
       " ('armies', 2),\n",
       " ('army', 4),\n",
       " ('array', 2),\n",
       " ('arrival', 6),\n",
       " ('arrived', 4),\n",
       " ('arriving', 1),\n",
       " ('artists', 2),\n",
       " ('arts', 1),\n",
       " ('ascendance', 1),\n",
       " ('asgard', 2),\n",
       " ('asgardian', 2),\n",
       " ('asgardians', 2),\n",
       " ('aside', 2),\n",
       " ('ask', 1),\n",
       " ('asked', 2),\n",
       " ('asking', 1),\n",
       " ('asks', 1),\n",
       " ('aspect', 2),\n",
       " ('ass', 3),\n",
       " ('assault', 1),\n",
       " ('assemble', 1),\n",
       " ('assembled', 3),\n",
       " ('asshole', 1),\n",
       " ('assume', 1),\n",
       " ('astounding', 1),\n",
       " ('attached', 1),\n",
       " ('attack', 2),\n",
       " ('attacked', 1),\n",
       " ('attackers', 1),\n",
       " ('attained', 1),\n",
       " ('attempt', 2),\n",
       " ('attempted', 1),\n",
       " ('attempting', 1),\n",
       " ('attempts', 1),\n",
       " ('attend', 1),\n",
       " ('attention', 5),\n",
       " ('attila', 1),\n",
       " ('attuned', 1),\n",
       " ('audacious', 1),\n",
       " ('audience', 13),\n",
       " ('audiences', 4),\n",
       " ('august', 1),\n",
       " ('available', 1),\n",
       " ('avenger', 4),\n",
       " ('avengers', 130),\n",
       " ('average', 2),\n",
       " ('avidly', 1),\n",
       " ('avoid', 2),\n",
       " ('avoided', 1),\n",
       " ('avoids', 1),\n",
       " ('awaited', 1),\n",
       " ('award', 2),\n",
       " ('awards', 1),\n",
       " ('away', 6),\n",
       " ('awe', 1),\n",
       " ('awesome', 2),\n",
       " ('awkward', 1),\n",
       " ('background', 2),\n",
       " ('backlash', 1),\n",
       " ('backstory', 6),\n",
       " ('bad', 9),\n",
       " ('badass', 1),\n",
       " ('baddie', 2),\n",
       " ('baddies', 1),\n",
       " ('bag', 1),\n",
       " ('baked', 1),\n",
       " ('balance', 9),\n",
       " ('balanced', 4),\n",
       " ('balances', 2),\n",
       " ('balancing', 3),\n",
       " ('ball', 1),\n",
       " ('bam', 1),\n",
       " ('bang', 2),\n",
       " ('bank', 1),\n",
       " ('banner', 7),\n",
       " ('banter', 4),\n",
       " ('bare', 1),\n",
       " ('barnes', 1),\n",
       " ('barrel', 1),\n",
       " ('based', 5),\n",
       " ('baseline', 1),\n",
       " ('bash', 1),\n",
       " ('basic', 3),\n",
       " ('basically', 4),\n",
       " ('basis', 1),\n",
       " ('bathroom', 2),\n",
       " ('batman', 2),\n",
       " ('battle', 9),\n",
       " ('battlefield', 2),\n",
       " ('battles', 7),\n",
       " ('battling', 1),\n",
       " ('baubles', 1),\n",
       " ('bautista', 6),\n",
       " ('bay', 1),\n",
       " ('bear', 1),\n",
       " ('beard', 1),\n",
       " ('bearing', 1),\n",
       " ('beaten', 1),\n",
       " ('beats', 3),\n",
       " ('beautiful', 1),\n",
       " ('beauty', 1),\n",
       " ('beefy', 1),\n",
       " ('beer', 1),\n",
       " ('beg', 1),\n",
       " ('began', 1),\n",
       " ('begin', 1),\n",
       " ('beginning', 3),\n",
       " ('begins', 3),\n",
       " ('begun', 1),\n",
       " ('believable', 1),\n",
       " ('believe', 4),\n",
       " ('believer', 1),\n",
       " ('believers', 1),\n",
       " ('believes', 3),\n",
       " ('bell', 1),\n",
       " ('belongs', 1),\n",
       " ('beloved', 1),\n",
       " ('ben', 1),\n",
       " ('beneath', 1),\n",
       " ('benedict', 14),\n",
       " ('benefits', 1),\n",
       " ('benevolent', 1),\n",
       " ('benicio', 1),\n",
       " ('best', 17),\n",
       " ('bet', 1),\n",
       " ('bettany', 10),\n",
       " ('better', 2),\n",
       " ('big', 24),\n",
       " ('bigger', 6),\n",
       " ('biggest', 2),\n",
       " ('billboards', 1),\n",
       " ('billion', 1),\n",
       " ('bind', 1),\n",
       " ('birth', 1),\n",
       " ('birthday', 1),\n",
       " ('bit', 8),\n",
       " ('bits', 2),\n",
       " ('bitter', 1),\n",
       " ('black', 35),\n",
       " ('blade', 2),\n",
       " ('blatant', 1),\n",
       " ('bleakest', 1),\n",
       " ('blended', 1),\n",
       " ('blitzkrieg', 1),\n",
       " ('bloated', 3),\n",
       " ('blockbuster', 3),\n",
       " ('blonde', 1),\n",
       " ('bloom', 1),\n",
       " ('blowing', 1),\n",
       " ('blue', 3),\n",
       " ('blunt', 2),\n",
       " ('board', 1),\n",
       " ('bodies', 1),\n",
       " ('body', 1),\n",
       " ('bolts', 1),\n",
       " ('bona', 1),\n",
       " ('bond', 1),\n",
       " ('book', 19),\n",
       " ('books', 5),\n",
       " ('boom', 1),\n",
       " ('borders', 1),\n",
       " ('bored', 1),\n",
       " ('borrowed', 1),\n",
       " ('boseman', 11),\n",
       " ('bother', 1),\n",
       " ('bothers', 1),\n",
       " ('bounces', 1),\n",
       " ('bound', 1),\n",
       " ('boundaries', 1),\n",
       " ('box', 4),\n",
       " ('boy', 1),\n",
       " ('bradley', 6),\n",
       " ('brain', 1),\n",
       " ('brand', 4),\n",
       " ('brash', 1),\n",
       " ('brave', 2),\n",
       " ('braveheart', 1),\n",
       " ('breads', 1),\n",
       " ('breadth', 1),\n",
       " ('break', 2),\n",
       " ('breakneck', 1),\n",
       " ('breakout', 1),\n",
       " ('breaks', 1),\n",
       " ('breath', 2),\n",
       " ('breathe', 4),\n",
       " ('breathing', 1),\n",
       " ('breathless', 1),\n",
       " ('breathtaking', 1),\n",
       " ('brief', 1),\n",
       " ('brighter', 1),\n",
       " ('brilliance', 1),\n",
       " ('brilliant', 1),\n",
       " ('bring', 3),\n",
       " ('bringing', 2),\n",
       " ('brings', 4),\n",
       " ('brio', 1),\n",
       " ('broken', 1),\n",
       " ('brolin', 27),\n",
       " ('brooding', 1),\n",
       " ('brother', 2),\n",
       " ('brothers', 9),\n",
       " ('brought', 2),\n",
       " ('bruce', 6),\n",
       " ('brue', 1),\n",
       " ('brutal', 1),\n",
       " ('brute', 1),\n",
       " ('buatista', 1),\n",
       " ('bucky', 1),\n",
       " ('budget', 2),\n",
       " ('build', 2),\n",
       " ('building', 7),\n",
       " ('buildup', 3),\n",
       " ('buildups', 1),\n",
       " ('built', 2),\n",
       " ('bulging', 1),\n",
       " ('bulk', 1),\n",
       " ('bump', 1),\n",
       " ('bumped', 1),\n",
       " ('bunch', 1),\n",
       " ('burn', 1),\n",
       " ('bus', 2),\n",
       " ('business', 2),\n",
       " ('butt', 1),\n",
       " ('butter', 1),\n",
       " ('buy', 1),\n",
       " ('buying', 1),\n",
       " ('c', 4),\n",
       " ('caesar', 1),\n",
       " ('caffeine', 1),\n",
       " ('cake', 1),\n",
       " ('calculated', 1),\n",
       " ('calculating', 1),\n",
       " ('called', 4),\n",
       " ('calling', 1),\n",
       " ('came', 10),\n",
       " ('cameo', 1),\n",
       " ('camera', 2),\n",
       " ('campaign', 1),\n",
       " ('candle', 1),\n",
       " ('canny', 1),\n",
       " ('cap', 1),\n",
       " ('capable', 3),\n",
       " ('capital', 1),\n",
       " ('capitalism', 1),\n",
       " ('captain', 33),\n",
       " ('capture', 1),\n",
       " ('card', 1),\n",
       " ('care', 5),\n",
       " ('cared', 1),\n",
       " ('careens', 1),\n",
       " ('careful', 2),\n",
       " ('carefully', 1),\n",
       " ('cares', 1),\n",
       " ('caring', 1),\n",
       " ('carnage', 2),\n",
       " ('carried', 1),\n",
       " ('carrier', 2),\n",
       " ('carries', 1),\n",
       " ('carry', 3),\n",
       " ('case', 4),\n",
       " ('cash', 1),\n",
       " ('cast', 13),\n",
       " ('casualty', 1),\n",
       " ('cat', 1),\n",
       " ('catalyze', 1),\n",
       " ('catch', 1),\n",
       " ('cause', 1),\n",
       " ('caused', 1),\n",
       " ('caveat', 1),\n",
       " ('celebration', 1),\n",
       " ('center', 2),\n",
       " ('central', 4),\n",
       " ('centre', 1),\n",
       " ('centred', 1),\n",
       " ('century', 1),\n",
       " ('certain', 10),\n",
       " ('certainly', 3),\n",
       " ('cg', 1),\n",
       " ('cgi', 6),\n",
       " ('chadwick', 11),\n",
       " ('chain', 1),\n",
       " ('challa', 2),\n",
       " ('challenge', 3),\n",
       " ('challenges', 2),\n",
       " ('chan', 1),\n",
       " ('chance', 3),\n",
       " ('change', 3),\n",
       " ('changed', 1),\n",
       " ('changer', 1),\n",
       " ('changes', 2),\n",
       " ('changing', 1),\n",
       " ('channel', 2),\n",
       " ('chaos', 2),\n",
       " ('chapter', 1),\n",
       " ('chapters', 2),\n",
       " ('character', 38),\n",
       " ('characterisation', 1),\n",
       " ('characteristics', 1),\n",
       " ('characters', 74),\n",
       " ('charge', 4),\n",
       " ('charlie', 1),\n",
       " ('chase', 1),\n",
       " ('chases', 1),\n",
       " ('chasing', 3),\n",
       " ('cheadle', 2),\n",
       " ('cheat', 1),\n",
       " ('cheated', 1),\n",
       " ('cheering', 1),\n",
       " ('chef', 1),\n",
       " ('chief', 1),\n",
       " ('child', 1),\n",
       " ('childhood', 1),\n",
       " ('children', 1),\n",
       " ('chill', 1),\n",
       " ('chills', 2),\n",
       " ('chin', 1),\n",
       " ('choice', 2),\n",
       " ('choices', 1),\n",
       " ('choreographing', 1),\n",
       " ('chris', 36),\n",
       " ('christopher', 5),\n",
       " ('chuckle', 1),\n",
       " ('chunk', 4),\n",
       " ('cinema', 2),\n",
       " ('cinematic', 25),\n",
       " ('cinematography', 2),\n",
       " ('circumstances', 1),\n",
       " ('cite', 1),\n",
       " ('cities', 1),\n",
       " ('city', 1),\n",
       " ('civil', 13),\n",
       " ('claim', 1),\n",
       " ('clamoring', 1),\n",
       " ('clangs', 1),\n",
       " ('clapping', 1),\n",
       " ('clarity', 1),\n",
       " ('clash', 4),\n",
       " ('clashes', 1),\n",
       " ('clashing', 1),\n",
       " ('class', 1),\n",
       " ('classic', 2),\n",
       " ('classics', 1),\n",
       " ('clear', 2),\n",
       " ('clearing', 1),\n",
       " ('clearly', 2),\n",
       " ('clever', 2),\n",
       " ('cliffhanger', 4),\n",
       " ('climactic', 1),\n",
       " ('climaxes', 1),\n",
       " ('climbers', 1),\n",
       " ('clips', 1),\n",
       " ('cloaked', 1),\n",
       " ('clock', 1),\n",
       " ('clocks', 1),\n",
       " ('close', 6),\n",
       " ('closer', 3),\n",
       " ('coalesce', 1),\n",
       " ('coasters', 2),\n",
       " ('coattails', 1),\n",
       " ('coffee', 1),\n",
       " ('coffers', 1),\n",
       " ('coherent', 1),\n",
       " ('cohesive', 2),\n",
       " ('cold', 1),\n",
       " ('colleagues', 1),\n",
       " ('collect', 10),\n",
       " ('collected', 1),\n",
       " ('collection', 2),\n",
       " ('collective', 1),\n",
       " ('collects', 2),\n",
       " ('color', 1),\n",
       " ('colour', 1),\n",
       " ('coloured', 1),\n",
       " ('colours', 1),\n",
       " ('combat', 1),\n",
       " ('combination', 2),\n",
       " ('combine', 1),\n",
       " ('combined', 2),\n",
       " ('combiner', 1),\n",
       " ('combos', 1),\n",
       " ('come', 28),\n",
       " ('comedy', 4),\n",
       " ('comes', 11),\n",
       " ('comforting', 1),\n",
       " ('comic', 26),\n",
       " ('comics', 8),\n",
       " ('coming', 9),\n",
       " ('commander', 1),\n",
       " ('commendable', 1),\n",
       " ('commercial', 3),\n",
       " ('commitment', 2),\n",
       " ('community', 1),\n",
       " ('company', 2),\n",
       " ('comparatively', 1),\n",
       " ('compare', 1),\n",
       " ('compatriots', 1),\n",
       " ('compelling', 2),\n",
       " ('complaining', 1),\n",
       " ('complaint', 1),\n",
       " ('complaints', 1),\n",
       " ('complete', 7),\n",
       " ('completed', 1),\n",
       " ('completely', 2),\n",
       " ('complex', 6),\n",
       " ('complicated', 1),\n",
       " ('composer', 1),\n",
       " ('concepts', 2),\n",
       " ('concern', 3),\n",
       " ('concerned', 4),\n",
       " ('concerns', 5),\n",
       " ('conclude', 1),\n",
       " ('concludes', 3),\n",
       " ('conclusion', 2),\n",
       " ('concoct', 1),\n",
       " ('conflict', 2),\n",
       " ('conflicted', 1),\n",
       " ('conflicts', 2),\n",
       " ('confrontation', 1),\n",
       " ('confrontations', 2),\n",
       " ('confused', 3),\n",
       " ('confusion', 1),\n",
       " ('congratulate', 1),\n",
       " ('connect', 1),\n",
       " ('connecting', 1),\n",
       " ('connection', 3),\n",
       " ('connections', 1),\n",
       " ('consciousness', 1),\n",
       " ('consent', 1),\n",
       " ('consequence', 2),\n",
       " ('consequences', 6),\n",
       " ('conservative', 1),\n",
       " ('considerable', 1),\n",
       " ('considered', 2),\n",
       " ('considering', 1),\n",
       " ('consistency', 1),\n",
       " ('conspiracy', 1),\n",
       " ('constant', 2),\n",
       " ('constantly', 2),\n",
       " ('constellations', 1),\n",
       " ('constituent', 1),\n",
       " ('consummate', 1),\n",
       " ('contemplative', 2),\n",
       " ('contemporary', 1),\n",
       " ('content', 1),\n",
       " ('context', 1),\n",
       " ('continuation', 1),\n",
       " ('continue', 1),\n",
       " ('continues', 2),\n",
       " ('contracts', 2),\n",
       " ('contributed', 1),\n",
       " ('control', 2),\n",
       " ('conventions', 1),\n",
       " ('convergence', 2),\n",
       " ('conversation', 2),\n",
       " ('convert', 1),\n",
       " ('convoluted', 1),\n",
       " ('cooking', 1),\n",
       " ('cool', 2),\n",
       " ('cooper', 6),\n",
       " ('coping', 1),\n",
       " ('copious', 1),\n",
       " ('copying', 1),\n",
       " ('core', 3),\n",
       " ('corporate', 1),\n",
       " ('correctness', 1),\n",
       " ('cosmic', 2),\n",
       " ('cosmos', 5),\n",
       " ('cosplay', 1),\n",
       " ('couldn', 2),\n",
       " ('coulson', 1),\n",
       " ('counterpart', 1),\n",
       " ('counterparts', 1),\n",
       " ('countless', 4),\n",
       " ('county', 1),\n",
       " ('couple', 1),\n",
       " ('couples', 1),\n",
       " ('course', 10),\n",
       " ('cover', 1),\n",
       " ('covers', 1),\n",
       " ('cracking', 1),\n",
       " ('craft', 3),\n",
       " ('cram', 1),\n",
       " ('crams', 1),\n",
       " ('cranking', 1),\n",
       " ('crap', 1),\n",
       " ('crash', 2),\n",
       " ('crashing', 2),\n",
       " ('create', 3),\n",
       " ('created', 5),\n",
       " ('creates', 1),\n",
       " ('creating', 3),\n",
       " ('creation', 2),\n",
       " ('creative', 1),\n",
       " ('creature', 4),\n",
       " ('credit', 7),\n",
       " ('credits', 6),\n",
       " ('crew', 2),\n",
       " ('cried', 1),\n",
       " ('crisis', 1),\n",
       " ('critic', 1),\n",
       " ('critical', 2),\n",
       " ('criticism', 1),\n",
       " ('criticize', 1),\n",
       " ('croods', 1),\n",
       " ('cross', 1),\n",
       " ('crosses', 1),\n",
       " ('crossover', 3),\n",
       " ('crossovers', 3),\n",
       " ('crucial', 1),\n",
       " ('crushing', 1),\n",
       " ('cryptic', 1),\n",
       " ('crystals', 1),\n",
       " ('culling', 2),\n",
       " ('culminated', 1),\n",
       " ('culminates', 1),\n",
       " ('culmination', 8),\n",
       " ('cultural', 1),\n",
       " ('cumberbatch', 13),\n",
       " ('cup', 1),\n",
       " ('curious', 2),\n",
       " ('cut', 5),\n",
       " ('cuts', 1),\n",
       " ('cutting', 2),\n",
       " ('cycle', 1),\n",
       " ('d', 4),\n",
       " ('daddy', 1),\n",
       " ('damaged', 1),\n",
       " ('damn', 1),\n",
       " ('danai', 1),\n",
       " ('danger', 1),\n",
       " ('dare', 1),\n",
       " ('daring', 1),\n",
       " ('dark', 7),\n",
       " ('darker', 3),\n",
       " ('darkness', 3),\n",
       " ('dash', 1),\n",
       " ('date', 6),\n",
       " ('daughter', 6),\n",
       " ('daunting', 2),\n",
       " ('dave', 5),\n",
       " ('day', 3),\n",
       " ('dc', 8),\n",
       " ('dceu', 1),\n",
       " ('dead', 4),\n",
       " ('deadly', 3),\n",
       " ('deadpool', 1),\n",
       " ('dealing', 1),\n",
       " ('dealt', 1),\n",
       " ('death', 3),\n",
       " ('decade', 6),\n",
       " ('decades', 1),\n",
       " ('decide', 1),\n",
       " ('decidedly', 1),\n",
       " ('decimated', 1),\n",
       " ('decks', 1),\n",
       " ('decreased', 1),\n",
       " ('dedicated', 1),\n",
       " ('deduced', 1),\n",
       " ('deep', 3),\n",
       " ('deeper', 3),\n",
       " ('deepest', 1),\n",
       " ('defeat', 2),\n",
       " ('defeated', 2),\n",
       " ('defend', 3),\n",
       " ('defending', 1),\n",
       " ('defined', 1),\n",
       " ('defining', 2),\n",
       " ('definitely', 2),\n",
       " ('degree', 1),\n",
       " ('del', 1),\n",
       " ('delight', 1),\n",
       " ('delights', 1),\n",
       " ('deliver', 7),\n",
       " ('delivered', 1),\n",
       " ('deliveries', 1),\n",
       " ('delivering', 1),\n",
       " ('delivers', 2),\n",
       " ('delivery', 2),\n",
       " ('demise', 1),\n",
       " ('denizens', 1),\n",
       " ('dense', 1),\n",
       " ('denying', 1),\n",
       " ('department', 1),\n",
       " ('departs', 1),\n",
       " ('depending', 2),\n",
       " ('depends', 1),\n",
       " ('depressed', 1),\n",
       " ('depressing', 1),\n",
       " ('depth', 7),\n",
       " ('depths', 1),\n",
       " ('descending', 1),\n",
       " ('described', 1),\n",
       " ('deserves', 3),\n",
       " ('design', 1),\n",
       " ('designed', 1),\n",
       " ('designs', 1),\n",
       " ('desire', 2),\n",
       " ('desperate', 1),\n",
       " ('desperately', 2),\n",
       " ('despise', 1),\n",
       " ('despite', 11),\n",
       " ('despot', 1),\n",
       " ('destroy', 2),\n",
       " ('destroyed', 2),\n",
       " ('destroyer', 1),\n",
       " ('destroying', 1),\n",
       " ('destruction', 1),\n",
       " ('destructive', 1),\n",
       " ('details', 1),\n",
       " ('determined', 2),\n",
       " ('determining', 1),\n",
       " ('develop', 3),\n",
       " ('developed', 3),\n",
       " ('developing', 1),\n",
       " ('development', 4),\n",
       " ('developments', 1),\n",
       " ('devoted', 2),\n",
       " ('devotion', 1),\n",
       " ('dialogue', 1),\n",
       " ('did', 7),\n",
       " ('didn', 10),\n",
       " ('die', 3),\n",
       " ('dies', 1),\n",
       " ('diesel', 4),\n",
       " ('difference', 2),\n",
       " ('different', 25),\n",
       " ('difficult', 7),\n",
       " ('difficulties', 1),\n",
       " ('digital', 1),\n",
       " ('dilemma', 1),\n",
       " ('dimensions', 1),\n",
       " ('dinklage', 5),\n",
       " ('direct', 1),\n",
       " ('directed', 4),\n",
       " ('directing', 2),\n",
       " ('direction', 1),\n",
       " ('directions', 2),\n",
       " ('director', 3),\n",
       " ('directors', 11),\n",
       " ('disappeared', 1),\n",
       " ('disappearing', 1),\n",
       " ('disappoint', 1),\n",
       " ('disappointed', 3),\n",
       " ('disappointing', 1),\n",
       " ('disappointingly', 1),\n",
       " ('disbanded', 2),\n",
       " ('discipline', 2),\n",
       " ('discourse', 1),\n",
       " ('discovered', 1),\n",
       " ('discuss', 1),\n",
       " ('discussing', 1),\n",
       " ('discussion', 3),\n",
       " ('disguised', 1),\n",
       " ('dislike', 1),\n",
       " ('disney', 5),\n",
       " ('disparate', 1),\n",
       " ('dispatches', 1),\n",
       " ('dispel', 1),\n",
       " ('dispensed', 1),\n",
       " ('display', 2),\n",
       " ('displayed', 1),\n",
       " ('disposal', 1),\n",
       " ('disproportionately', 1),\n",
       " ('disquieting', 1),\n",
       " ('distinct', 2),\n",
       " ('distract', 4),\n",
       " ('distracting', 1),\n",
       " ('distraction', 1),\n",
       " ('distractions', 1),\n",
       " ('distressingly', 1),\n",
       " ('distributed', 1),\n",
       " ('ditch', 1),\n",
       " ('ditto', 1),\n",
       " ('dive', 2),\n",
       " ('diverse', 1),\n",
       " ('divided', 1),\n",
       " ('divides', 1),\n",
       " ('diving', 1),\n",
       " ('divisive', 1),\n",
       " ('divulged', 1),\n",
       " ('dizzying', 1),\n",
       " ('dna', 1),\n",
       " ('doc', 4),\n",
       " ('doctor', 17),\n",
       " ('does', 23),\n",
       " ('doesn', 12),\n",
       " ('dog', 2),\n",
       " ('doing', 4),\n",
       " ('dollars', 2),\n",
       " ('dome', 1),\n",
       " ('dominate', 1),\n",
       " ('dominion', 1),\n",
       " ('don', 31),\n",
       " ('doomed', 2),\n",
       " ('dose', 1),\n",
       " ('doubled', 1),\n",
       " ('doubt', 1),\n",
       " ('dough', 1),\n",
       " ('dour', 1),\n",
       " ('downbeat', 1),\n",
       " ('downey', 14),\n",
       " ('dozen', 2),\n",
       " ('dr', 13),\n",
       " ('drained', 1),\n",
       " ('drama', 2),\n",
       " ('dramatic', 4),\n",
       " ('draw', 3),\n",
       " ('drawn', 1),\n",
       " ('drax', 8),\n",
       " ('dream', 4),\n",
       " ('dreamed', 1),\n",
       " ('drink', 1),\n",
       " ('drive', 1),\n",
       " ('drop', 1),\n",
       " ('droppingly', 1),\n",
       " ('drops', 1),\n",
       " ('duel', 1),\n",
       " ('duke', 1),\n",
       " ('dull', 1),\n",
       " ('duo', 1),\n",
       " ('duties', 1),\n",
       " ('dwarf', 1),\n",
       " ('dying', 1),\n",
       " ('dynamite', 1),\n",
       " ('e', 1),\n",
       " ('eager', 1),\n",
       " ('eared', 1),\n",
       " ('earlier', 1),\n",
       " ('earliest', 1),\n",
       " ('early', 8),\n",
       " ('earning', 1),\n",
       " ('earns', 1),\n",
       " ('earth', 11),\n",
       " ('easier', 1),\n",
       " ('easily', 5),\n",
       " ('easter', 2),\n",
       " ('easy', 2),\n",
       " ('ebb', 1),\n",
       " ('ebbing', 1),\n",
       " ('ebenezer', 1),\n",
       " ('eco', 1),\n",
       " ('economical', 1),\n",
       " ('editing', 1),\n",
       " ('effect', 3),\n",
       " ('effectively', 1),\n",
       " ('effects', 9),\n",
       " ('effort', 5),\n",
       " ('efforts', 3),\n",
       " ('eggs', 2),\n",
       " ('ego', 1),\n",
       " ('egoes', 1),\n",
       " ('egos', 2),\n",
       " ('eighteen', 2),\n",
       " ('eitri', 2),\n",
       " ('elba', 1),\n",
       " ('element', 1),\n",
       " ('elemental', 1),\n",
       " ('elements', 8),\n",
       " ('elephant', 1),\n",
       " ('eliminate', 1),\n",
       " ('elizabeth', 8),\n",
       " ('elysium', 1),\n",
       " ('embroiled', 1),\n",
       " ('emerge', 2),\n",
       " ('emissaries', 1),\n",
       " ('emotional', 7),\n",
       " ('emotionally', 3),\n",
       " ('emotions', 2),\n",
       " ('empathy', 1),\n",
       " ('empire', 1),\n",
       " ('encapsulated', 1),\n",
       " ('encompasses', 1),\n",
       " ('encounter', 2),\n",
       " ('encountered', 1),\n",
       " ('end', 30),\n",
       " ('endeavor', 1),\n",
       " ('ended', 1),\n",
       " ('endgame', 5),\n",
       " ('ending', 11),\n",
       " ('endings', 2),\n",
       " ('endless', 1),\n",
       " ('ends', 5),\n",
       " ('enemy', 1),\n",
       " ('energy', 1),\n",
       " ('engage', 2),\n",
       " ('engaged', 1),\n",
       " ('engineered', 1),\n",
       " ('enhance', 1),\n",
       " ('enjoy', 6),\n",
       " ('enjoyable', 1),\n",
       " ('enjoyed', 3),\n",
       " ('enjoyment', 3),\n",
       " ('enormous', 5),\n",
       " ('ensemble', 6),\n",
       " ('enterprise', 1),\n",
       " ('entertaining', 3),\n",
       " ('entertainment', 2),\n",
       " ('enthusiasm', 1),\n",
       " ('entire', 6),\n",
       " ('entirely', 1),\n",
       " ('entities', 1),\n",
       " ('entity', 2),\n",
       " ('entries', 2),\n",
       " ('entry', 4),\n",
       " ('envelope', 1),\n",
       " ('epic', 6),\n",
       " ('epically', 1),\n",
       " ('equity', 1),\n",
       " ('era', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "list(zip(cv.get_feature_names(), np.asarray(text_counts.sum(axis=0)).ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, movie['Rating'], test_size=0.33, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16x3352 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4664 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparse matrix\n",
    "X_train   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Model build and evaluation\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "expected  = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 9.00, expected: 9.00\n",
      "predicted: 9.00, expected: 7.00\n",
      "predicted: 7.00, expected: 7.00\n",
      "predicted: 9.00, expected: 9.00\n",
      "predicted: 9.00, expected: 9.00\n",
      "predicted: 9.00, expected: 9.00\n",
      "predicted: 7.00, expected: 9.00\n",
      "predicted: 7.00, expected: 7.00\n"
     ]
    }
   ],
   "source": [
    "for p, e in zip(predicted, expected):\n",
    "    print(f'predicted: {p:.2f}, expected: {e:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(predicted,expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[2 1]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_true=expected, y_pred=predicted)\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The diagonal elements show the number of correct classifications for each class i.e. Here for example 4,2 are correct classifications.\n",
    "2. The off-diagonal elements provides the misclassifications. Here for example both 1's are wrong classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization using WordNetLemmatizer:\n",
    ">The WordNetLemmatizer class has a method called lemmatize which takes as arguments a word to lemmatize as well as what part of speech the word happens to be i.e. noun, verb, adverb, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some vague references to things that happen and the usual discussion about plot elements available via trailers and pre-release clips, but I have tried to remain as Âspoiler-liteÂ as possible\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize a sentence\n",
    "print(lemmatizer.lemmatize(str(movie['Review'][0]).split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'some', 'vague', 'references', 'to', 'things', 'that', 'happen', 'and', 'the', 'usual', 'discussion', 'about', 'plot', 'elements', 'available', 'via', 'trailers', 'and', 'pre-release', 'clips', ',', 'but', 'I', 'have', 'tried', 'to', 'remain', 'as', '\\x93spoiler-lite\\x94', 'as', 'possible']\n",
      "There are some vague reference to thing that happen and the usual discussion about plot element available via trailer and pre-release clip , but I have tried to remain a Âspoiler-liteÂ a possible\n"
     ]
    }
   ],
   "source": [
    "## Define the sentence to be lemmatized\n",
    "sentence = lemmatizer.lemmatize(str(movie['Review'][0]).split('.')[0])\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here if we see some words like 'are' are not lemmatized as expected. Hence we should consider parts of speech while lemmatizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some vague references to things that happen and the usual discussion about plot elements available via trailers and pre-release clips, but I have tried to remain as Âspoiler-liteÂ as possible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There be some vague reference to thing that happen and the usual discussion about plot element available via trailer and pre-release clip but I have try to remain as \\x93spoiler-lite\\x94 a possible'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define function to lemmatize each word with its POS tag\n",
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "# Lemmatize\n",
    "sentence = lemmatizer.lemmatize(str(movie['Review'][0]).split('.')[0])\n",
    "print(sentence)\n",
    "lemmatize_with_postag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK provides various Stemmer Algorithms. Here we implemented LancasterStemmer and SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ther\n",
      "ar\n",
      "som\n",
      "vagu\n",
      "ref\n",
      "to\n",
      "thing\n",
      "that\n",
      "hap\n",
      "and\n",
      "the\n",
      "us\n",
      "discuss\n",
      "about\n",
      "plot\n",
      "el\n",
      "avail\n",
      "via\n",
      "trail\n",
      "and\n",
      "pre-releas\n",
      "clip\n",
      ",\n",
      "but\n",
      "i\n",
      "hav\n",
      "tri\n",
      "to\n",
      "remain\n",
      "as\n",
      "Âspoiler-liteÂ\n",
      "as\n",
      "poss\n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    print(lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "some\n",
      "vagu\n",
      "refer\n",
      "to\n",
      "thing\n",
      "that\n",
      "happen\n",
      "and\n",
      "the\n",
      "usual\n",
      "discuss\n",
      "about\n",
      "plot\n",
      "element\n",
      "avail\n",
      "via\n",
      "trailer\n",
      "and\n",
      "pre-releas\n",
      "clip\n",
      ",\n",
      "but\n",
      "i\n",
      "have\n",
      "tri\n",
      "to\n",
      "remain\n",
      "as\n",
      "Âspoiler-liteÂ\n",
      "as\n",
      "possibl\n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    print(snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that each stemming algorithm provides a different output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Analysis on two sets of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(\"ignore\") #Code to ignore warnings from getting displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from two datasets\n",
    "We did sentiment analysis on Avengers.csv , now we will do similarity with review for Joker movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Reviewer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>There are some vague references to things that...</td>\n",
       "      <td>James Berardinelli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>It must have taken a superhuman effort to pull...</td>\n",
       "      <td>JEFFREY M. ANDERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Considered on its own, as a single, nearly 2-h...</td>\n",
       "      <td>A.O. Scott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>If a movie can be defined by the strength of i...</td>\n",
       "      <td>Louise Keller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Ten years and 18 Marvel movies; 19 now, with t...</td>\n",
       "      <td>MaryAnn Johanson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>I try to write things that are spoiler free (f...</td>\n",
       "      <td>Nathaniel R. Mitchell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>Thanos (Josh Brolin) has arrivedÂ",
       "and he wants ...</td>\n",
       "      <td>JP Roscoe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Listing the cast members may take longer than ...</td>\n",
       "      <td>Doug Fisher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>For their 19th film, the Marvel Cinematic Univ...</td>\n",
       "      <td>Brian Orndorf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>Even with a very large cast of characters, thi...</td>\n",
       "      <td>Anthony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Now that we have a film with nearly every Marv...</td>\n",
       "      <td>Marc Eastman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>This is a film a decade in the offing, one tha...</td>\n",
       "      <td>Kyle Saubert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>ThereÂs an urge to overpraise Avengers: Infini...</td>\n",
       "      <td>Robert Vaux</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rating                                             Review  \\\n",
       "0        9  There are some vague references to things that...   \n",
       "1        7  It must have taken a superhuman effort to pull...   \n",
       "2        7  Considered on its own, as a single, nearly 2-h...   \n",
       "3        8  If a movie can be defined by the strength of i...   \n",
       "4        7  Ten years and 18 Marvel movies; 19 now, with t...   \n",
       "5        9  I try to write things that are spoiler free (f...   \n",
       "6        9  Thanos (Josh Brolin) has arrived\n",
       "and he wants ...   \n",
       "7        7  Listing the cast members may take longer than ...   \n",
       "8        9  For their 19th film, the Marvel Cinematic Univ...   \n",
       "9        7  Even with a very large cast of characters, thi...   \n",
       "10       6  Now that we have a film with nearly every Marv...   \n",
       "11       7  This is a film a decade in the offing, one tha...   \n",
       "12       8  ThereÂs an urge to overpraise Avengers: Infini...   \n",
       "\n",
       "                 Reviewer  \n",
       "0      James Berardinelli  \n",
       "1     JEFFREY M. ANDERSON  \n",
       "2             Â A.O. Scott  \n",
       "3           Louise Keller  \n",
       "4        MaryAnn Johanson  \n",
       "5   Nathaniel R. Mitchell  \n",
       "6               JP Roscoe  \n",
       "7             Doug Fisher  \n",
       "8           Brian Orndorf  \n",
       "9                 Anthony  \n",
       "10           Marc Eastman  \n",
       "11           Kyle Saubert  \n",
       "12            Robert Vaux  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie1=pd.read_csv('Avengers.csv',delimiter=',',encoding='unicode_escape')\n",
    "movie2=pd.read_csv('Jokers.csv',delimiter=',',encoding='unicode_escape')\n",
    "movie1.head(13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Reviewer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>In the Batman universe, there are two kinds of...</td>\n",
       "      <td>James Berardinelli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.5</td>\n",
       "      <td>ÂJokerÂ is a comic book movie as filtered thro...</td>\n",
       "      <td>Jeffrey M. Anderson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Since its debut a few weeks ago at the Venice ...</td>\n",
       "      <td>A.O. Scott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>Edgy and darkly disturbing, Joker takes flight...</td>\n",
       "      <td>Louise Keller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>Alas, the only remotely entertaining or even m...</td>\n",
       "      <td>MaryAnn Johanson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review  \\\n",
       "0     4.5  In the Batman universe, there are two kinds of...   \n",
       "1     3.5  ÂJokerÂ is a comic book movie as filtered thro...   \n",
       "2     4.0  Since its debut a few weeks ago at the Venice ...   \n",
       "3     4.5  Edgy and darkly disturbing, Joker takes flight...   \n",
       "4     1.5  Alas, the only remotely entertaining or even m...   \n",
       "\n",
       "              Reviewer  \n",
       "0   James Berardinelli  \n",
       "1  Jeffrey M. Anderson  \n",
       "2          Â A.O. Scott  \n",
       "3        Louise Keller  \n",
       "4     MaryAnn Johanson  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert reviews to nlp documents type for both movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie1['Review_nlp'] = movie1['Review'].apply(lambda x: nlp((x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie2['Review_nlp'] = movie2['Review'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity of reviews using spacy.\n",
    "#### Calculating similarity score between each review from first set with each review from second set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt={} #empty dictionary to store similarity score per reviewer pair\n",
    "for rev1,r1 in zip(movie1['Reviewer'],movie1['Review_nlp']):\n",
    "    for rev2,r2 in zip(movie2['Reviewer'],movie2['Review_nlp']):\n",
    "        dt[rev1,rev2]=r1.similarity(r2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations of accuracy of similarity score calculated above\n",
    "#### Since after above calculations similarity scores are high,we are considering similarity if score is greater than 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James Berardinelli', 'James Berardinelli'] 0.9871946870650147\n",
      "['JEFFREY M. ANDERSON', 'Jeffrey M. Anderson'] 0.9401306685105196\n",
      "['\\xa0A.O. Scott', '\\xa0A.O. Scott'] 0.9770719725418836\n",
      "['Louise Keller', 'Louise Keller'] 0.9429127320593583\n",
      "['MaryAnn Johanson', 'MaryAnn Johanson'] 0.9691359943016407\n",
      "['Nathaniel R. Mitchell', 'Nathaniel R. Mitchell'] 0.9745974238812629\n",
      "['JP Roscoe', 'JP Roscoe'] 0.9794450181438646\n",
      "['Doug Fisher', 'Doug Fisher'] 0.9565542129216947\n",
      "['Brian Orndorf', 'Brian Orndorf'] 0.982109863278267\n",
      "['Anthony', 'Anthony'] 0.9640639550547744\n",
      "['Marc Eastman', 'Marc Eastman'] 0.9722217228038148\n",
      "['Kyle Saubert', 'Kyle Saubert'] 0.9707529435676392\n"
     ]
    }
   ],
   "source": [
    "correct_pos=0\n",
    "correct_neg=0\n",
    "false_pos=0\n",
    "false_neg=0\n",
    "total=0\n",
    "for key,value in dt.items():\n",
    "    #print(list(key),value)\n",
    "    total+=1\n",
    "    if str(list(key)[0]).lower()==str(list(key)[1]).lower() and value*100 > 90:\n",
    "        correct_pos+=1\n",
    "        print(list(key),value)\n",
    "    elif str(list(key)[0]).lower()==str(list(key)[1]).lower() and value*100 < 90:\n",
    "        false_neg+=1\n",
    "    elif str(list(key)[0]).lower()!=str(list(key)[1]).lower() and value*100 > 90:\n",
    "        false_pos+=1\n",
    "    elif str(list(key)[0]).lower()!=str(list(key)[1]).lower() and value*100 < 90:\n",
    "        correct_neg+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct count 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct count\",correct_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive 239\n"
     ]
    }
   ],
   "source": [
    "print(\"False Positive\",false_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative 37\n"
     ]
    }
   ],
   "source": [
    "print(\"True Negative\",correct_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative 0\n",
      "Total Pairs 288\n"
     ]
    }
   ],
   "source": [
    "print(\"False Negative\",false_neg)\n",
    "print(\"Total Pairs\",total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score for how well the analysis predicted the same author.\n",
    "For computing the score we can see how many false positives and negatives vs actual positives the analyzer computed.\n",
    ">Accuracy Score=items classified correctly\\all items classified\n",
    "\n",
    "In our calculations items classified correctly will be correct positive(matching reviewers)+correct negative(low similarity for different reviewers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.01%\n"
     ]
    }
   ],
   "source": [
    "score=(correct_pos+correct_neg)/total\n",
    "print(f'{score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "From above similarity analysis we can conclude that:\n",
    "- There were no False Negative i.e algorithm was able to identify similarities in text of same reviewers from both sets.\n",
    "- For same reviewer , similarity score is greater than 94%.\n",
    "- But for different pair of reviewers also we got match i.e there were huge false positives.\n",
    "- This analysis though gave correct match for matching reviewers but there were false positives which creates a doubt on the accuracy of this algorithm.\n",
    "- One possible reason for that is, since we are using movie review and words and genre also influences the tone and language of review, there might have been similarity in words and sentiment used, which resulted in similarity among different reviewers.\n",
    "- Method we used will be based on the tagger, parser and NER and not on vectors as module we loaded  don't ship with word vectors and only use context-sensitive tensors.Hence, similarity prediction will get false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cosine formula to calculate similarity\n",
    "Cosine similarity calculates similarity by measuring the cosine of angle between two vectors.With cosine similarity, we need to convert sentences into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate similarity between two texts using cosine function\n",
    "def similar(X,Y):\n",
    "# Program to measure similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize \n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "\n",
    "    # tokenization \n",
    "    X_list = word_tokenize(X)  \n",
    "    Y_list = word_tokenize(Y) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    cosine=1 - cosine(l1, l2)\n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cos={}\n",
    "for rev1,r1 in zip(movie1['Reviewer'],movie1['Review_nlp']):\n",
    "    for rev2,r2 in zip(movie2['Reviewer'],movie2['Review_nlp']):\n",
    "        sim=similar(str(r1),str(r2))\n",
    "        dt_cos[rev1,rev2]=sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James Berardinelli', 'James Berardinelli'] 16.42436111940708\n",
      "['JEFFREY M. ANDERSON', 'Jeffrey M. Anderson'] 11.875241547680382\n",
      "['\\xa0A.O. Scott', '\\xa0A.O. Scott'] 16.59973446562184\n",
      "['Louise Keller', 'Louise Keller'] 11.59109244835429\n",
      "['MaryAnn Johanson', 'MaryAnn Johanson'] 14.199986979256662\n",
      "['Nathaniel R. Mitchell', 'Nathaniel R. Mitchell'] 24.531448232566976\n",
      "['JP Roscoe', 'JP Roscoe'] 15.992918923606148\n",
      "['Doug Fisher', 'Doug Fisher'] 12.353441144022892\n",
      "['Brian Orndorf', 'Brian Orndorf'] 15.59893159319926\n",
      "['Anthony', 'Anthony'] 26.272001939513757\n",
      "['Marc Eastman', 'Marc Eastman'] 19.132444176530793\n",
      "['Kyle Saubert', 'Kyle Saubert'] 21.958178920326855\n"
     ]
    }
   ],
   "source": [
    "for key,value in dt_cos.items():\n",
    "    #print(list(key),value)\n",
    "    if list(key)[0].lower()==list(key)[1].lower() :\n",
    "        print(list(key),value*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard method for similarity\n",
    "Jaccard similarity or intersection over union is defined as size of intersection divided by size of union of two sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaccard method to calculate similarity\n",
    "def get_jaccard_sim(str1, str2): \n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_jacc={}\n",
    "for rev1,r1 in zip(movie1['Reviewer'],movie1['Review_nlp']):\n",
    "    for rev2,r2 in zip(movie2['Reviewer'],movie2['Review_nlp']):\n",
    "        sim=get_jaccard_sim(str(r1),str(r2))\n",
    "        dt_jacc[rev1,rev2]=sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James Berardinelli', 'James Berardinelli'] 16.42436111940708\n",
      "['JEFFREY M. ANDERSON', 'Jeffrey M. Anderson'] 11.875241547680382\n",
      "['\\xa0A.O. Scott', '\\xa0A.O. Scott'] 16.59973446562184\n",
      "['Louise Keller', 'Louise Keller'] 11.59109244835429\n",
      "['MaryAnn Johanson', 'MaryAnn Johanson'] 14.199986979256662\n",
      "['Nathaniel R. Mitchell', 'Nathaniel R. Mitchell'] 24.531448232566976\n",
      "['JP Roscoe', 'JP Roscoe'] 15.992918923606148\n",
      "['Doug Fisher', 'Doug Fisher'] 12.353441144022892\n",
      "['Brian Orndorf', 'Brian Orndorf'] 15.59893159319926\n",
      "['Anthony', 'Anthony'] 26.272001939513757\n",
      "['Marc Eastman', 'Marc Eastman'] 19.132444176530793\n",
      "['Kyle Saubert', 'Kyle Saubert'] 21.958178920326855\n"
     ]
    }
   ],
   "source": [
    "for key,value in dt_cos.items():\n",
    "    #print(list(key),value)\n",
    "    if list(key)[0].lower()==list(key)[1].lower() :\n",
    "        print(list(key),value*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on Cosine and Jaccard method\n",
    "- Both Jaccard and cosine method has very low scores for similarity and we can not predict which pair of reviews were written by same reviewer.\n",
    "- Jaccard similarity takes only unique set of words for each sentence / document while cosine similarity takes total length of the vectors.\n",
    "- Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. For two product descriptions, it will be better to use Jaccard similarity as repetition of a word does not reduce their similarity.\n",
    "\n",
    "`Low similarity scores might be because both the movies are of different genres and might require different context for reviewing. However, reviewers often use similar words for reviewing a movie hence we get some similarity in text from both the sets.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency\n",
    "In class examples we used TextBlob to get word counts. Here we have used FreqDist from nltk library to get counts of word in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(word.lower() for word in word_tokenize(str(movie1['Review'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 79),\n",
       " ('.', 54),\n",
       " (',', 51),\n",
       " ('a', 40),\n",
       " ('of', 36),\n",
       " ('and', 34),\n",
       " ('to', 29),\n",
       " ('in', 20),\n",
       " ('(', 20),\n",
       " (')', 20)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many stop words. Taking the count after removing stop words and also punctuations.\n",
    "For punctuation we will import string library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    " \n",
    "stopwords_english = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'vague', 'references', 'things', 'happen', 'usual', 'discussion', 'plot', 'elements', 'available']\n"
     ]
    }
   ],
   "source": [
    "words_clean = []\n",
    "for word in word_tokenize(str(movie1['Review'][0])):\n",
    "    if word not in stopwords_english and word not in string.punctuation:\n",
    "        words_clean.append(word)\n",
    " \n",
    "print(words_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Infinity', 11),\n",
       " ('War', 11),\n",
       " ('movie', 10),\n",
       " ('The', 9),\n",
       " ('things', 7),\n",
       " ('film', 7),\n",
       " ('\\x96', 7),\n",
       " ('Avengers', 6),\n",
       " ('characters', 5),\n",
       " ('time', 5)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_new = FreqDist(words_clean)\n",
    "fdist_new.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation\n",
    "Using different variations of function covered in class and on the first review of movie Avenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = str(movie2['Review'][0]) #select first review of Joker movie\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt=blob.translate(from_lang='en',to='fr') #translate it to french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng=blob.translate(from_lang='fr',to='en') #translate back to french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_trns=nlp(str(eng)).similarity(nlp(str(blob))) #check with above translated text with original review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity after translation back to english with original review in english is :97.91985717566446%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity after translation back to english with original review in english is :{sim_trns*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : We see above that some meaning is lost in transalation hence similarity is not 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjectivity of text in review(value returned is between 0 and 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4364847725374041"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.subjectivity #Returns the subjectivity score of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams\n",
    "There are explicit functions for bigrams and trigrams in nltk, apart for ngram function in textblob library discussed in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = nltk.word_tokenize(str(movie1['Review'][0]))\n",
    "my_bigrams = nltk.bigrams(words)\n",
    "my_trigrams = nltk.trigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'are'),\n",
       " ('are', 'some'),\n",
       " ('some', 'vague'),\n",
       " ('vague', 'references'),\n",
       " ('references', 'to'),\n",
       " ('to', 'things'),\n",
       " ('things', 'that'),\n",
       " ('that', 'happen'),\n",
       " ('happen', 'and')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_bigrams)[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'are', 'some'),\n",
       " ('are', 'some', 'vague'),\n",
       " ('some', 'vague', 'references'),\n",
       " ('vague', 'references', 'to'),\n",
       " ('references', 'to', 'things'),\n",
       " ('to', 'things', 'that'),\n",
       " ('things', 'that', 'happen'),\n",
       " ('that', 'happen', 'and'),\n",
       " ('happen', 'and', 'the'),\n",
       " ('and', 'the', 'usual')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_trigrams)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment to check most common words in all the reviews of a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_movie1 = []\n",
    "punc=string.punctuation+'â'+'â'+'â' #string to hold punctuations that appear in text\n",
    "for line in (movie2['Review']):\n",
    "    for word in word_tokenize(line):\n",
    "        if word not in stopwords_english and word not in punc:\n",
    "            words_movie1.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Joker', 84),\n",
       " ('The', 67),\n",
       " ('Arthur', 63),\n",
       " ('I', 45),\n",
       " ('film', 44),\n",
       " ('movie', 43),\n",
       " (\"'s\", 42),\n",
       " ('Phoenix', 37),\n",
       " ('story', 29),\n",
       " ('one', 27),\n",
       " ('character', 27),\n",
       " ('Batman', 26),\n",
       " ('Phillips', 26),\n",
       " ('like', 26),\n",
       " ('he\\x92s', 25),\n",
       " ('even', 24),\n",
       " ('also', 24),\n",
       " ('It', 24),\n",
       " ('see', 23),\n",
       " ('He', 22),\n",
       " ('This', 21),\n",
       " ('Gotham', 21),\n",
       " ('comic', 19),\n",
       " ('It\\x92s', 19),\n",
       " ('Joaquin', 18),\n",
       " ('clown', 18),\n",
       " ('people', 18),\n",
       " ('book', 17),\n",
       " ('mother', 17),\n",
       " ('world', 17)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_movie1 = FreqDist(words_movie1) #Get the word count using FreqDist function\n",
    "word_count_movie1.most_common(30) #print top 30 words in all the reviews of movie Joker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
